{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "from bertopic import BERTopic\n",
    "import pandas as pd\n",
    "import torch\n",
    "from thinc.api import set_gpu_allocator, require_gpu\n",
    "\n",
    "# Controleer of GPU beschikbaar is\n",
    "if torch.cuda.is_available():\n",
    "    set_gpu_allocator(\"pytorch\")  # Stel de GPU-geheugenbeheerder in voor PyTorch\n",
    "    require_gpu(0)  # Vereis GPU 0\n",
    "    device = \"cuda\"\n",
    "    print(\"GPU wordt gebruikt!\")  # Gebruik GPU\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    print(\"CPU wordt gebruikt\")  # Gebruik CPU als er geen GPU is\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "import cupy as cp\n",
    "\n",
    "# Forceer het vrijmaken van geheugen\n",
    "cp.get_default_memory_pool().free_all_blocks()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Documenten voorbereiden",
   "id": "e93b1f72b3178062"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Lees de CSV\n",
    "df = pd.read_csv(\"/home/nena-meijer/PyCharmMiscProject/event_extraction/GEMINI_RESULTS_preprocessed.csv\")\n",
    "\n",
    "# Filter alleen rijen waar 'summary' een string is\n",
    "filtered_df = df[df['summary'].apply(lambda x: isinstance(x, str))]\n",
    "\n",
    "# Haal de tekst en datums op\n",
    "document_ids = filtered_df['document_id'].tolist()\n",
    "document_text = filtered_df['summary'].tolist()\n",
    "document_dates = pd.to_datetime(filtered_df['document_date_y'], errors='coerce')\n",
    "\n",
    "# Verwijder rijen zonder geldige datum\n",
    "valid_mask = document_dates.notna()\n",
    "document_text = [doc for doc, keep in zip(document_text, valid_mask) if keep]\n",
    "document_dates = document_dates[valid_mask].tolist()\n",
    "\n",
    "# Controle\n",
    "print(len(document_text), len(document_dates))"
   ],
   "id": "bd180fd8ab9e8d21",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Chunk de documenten",
   "id": "9d27ec10b3297c40"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer\n",
    "import pickle\n",
    "\n",
    "# Laad de tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"intfloat/multilingual-e5-large-instruct\")\n",
    "\n",
    "# Functie om tekst te chunkeren\n",
    "def chunk_text(text, tokenizer, max_length=512, overlap=50):\n",
    "    # Tokenize de tekst met truncatie\n",
    "    tokens = tokenizer.encode(text, truncation=True, padding=False, max_length=max_length)\n",
    "\n",
    "    # Start met een lege lijst voor de chunks\n",
    "    chunks = []\n",
    "\n",
    "    # Itereer door de tokens en maak chunks met overlap\n",
    "    for i in range(0, len(tokens), max_length - overlap):\n",
    "        chunk = tokens[i:i + max_length]  # Maak een chunk van max_length tokens\n",
    "        chunks.append(chunk)  # Voeg de chunk toe aan de lijst\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "# Lijst voor de chunks\n",
    "all_chunks = []\n",
    "\n",
    "# Tokenize en chunk elk document in de lijst document_text\n",
    "for idx, text in enumerate(document_text):\n",
    "    print(f\"Tokenizing document {idx + 1}/{len(document_text)}...\")\n",
    "    chunks = chunk_text(text, tokenizer, max_length=512, overlap=50)\n",
    "    all_chunks.append({\n",
    "        'document_id': idx,  # Document ID om het document te traceren\n",
    "        'chunks': chunks\n",
    "    })\n",
    "\n",
    "# Bestandsnaam voor de opgeslagen chunks\n",
    "chunks_file = '/home/nena-meijer/PyCharmMiscProject/topic_modelling/BERTopic/chunks/chunks_512_extra_preprocessing.pkl'\n",
    "\n",
    "# Opslaan van de chunks in een pickle-bestand\n",
    "with open(chunks_file, 'wb') as f:\n",
    "    pickle.dump(all_chunks, f)\n",
    "\n",
    "print(f\"Chunks zijn opgeslagen in: {chunks_file}\")"
   ],
   "id": "322bbcf2472642c0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Embeddings genereren van de chunks",
   "id": "11a675c99a943524"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Pre-calculate de embeddings\n",
    "import pickle\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Het model wordt uitgevoerd op: {device}\")\n",
    "\n",
    "# Laad het model en de tokenizer\n",
    "model_name = \"intfloat/multilingual-e5-large-instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# Functie voor het genereren van embeddings\n",
    "def generate_embeddings(chunks, tokenizer, model):\n",
    "    # Initialiseer een lijst om de embeddings op te slaan\n",
    "    embeddings = []\n",
    "\n",
    "    # Itereer over de chunks en genereer embeddings\n",
    "    for chunk in chunks:\n",
    "        text = tokenizer.decode(chunk, skip_special_tokens=True)\n",
    "        # Tokenize de chunk\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "\n",
    "        # Voer het model uit om de verborgen toestanden te krijgen\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            last_hidden_state = outputs.last_hidden_state\n",
    "\n",
    "        # We nemen de gemiddelde pooling van de laatste verborgen toestand\n",
    "        attention_mask = inputs[\"attention_mask\"]\n",
    "        last_hidden_state = last_hidden_state.masked_fill(~attention_mask.unsqueeze(-1).bool(), 0)\n",
    "        chunk_embedding = last_hidden_state.sum(dim=1) / attention_mask.sum(dim=1).unsqueeze(-1)\n",
    "\n",
    "        # Normaliseer de embedding\n",
    "        chunk_embedding = F.normalize(chunk_embedding, p=2, dim=1)\n",
    "\n",
    "        # Voeg de embedding toe aan de lijst\n",
    "        embeddings.append(chunk_embedding.squeeze().cpu().numpy())  # Verwijder batch dim en converteer naar numpy array\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "# Laad de chunks uit het eerder opgeslagen pickle-bestand\n",
    "chunks_file = '/home/nena-meijer/PyCharmMiscProject/topic_modelling/BERTopic/chunks/chunks_512_extra_preprocessing.pkl'\n",
    "\n",
    "with open(chunks_file, 'rb') as f:\n",
    "    loaded_chunks = pickle.load(f)\n",
    "\n",
    "# Lijst om de embeddings op te slaan\n",
    "all_embeddings = []\n",
    "\n",
    "# Genereer embeddings voor de chunks\n",
    "for doc in loaded_chunks:\n",
    "    document_id = doc['document_id']\n",
    "    chunks = doc['chunks']\n",
    "    print(f\"Genereer embeddings voor document {document_id + 1}...\")\n",
    "    document_embeddings = generate_embeddings(chunks, tokenizer, model)\n",
    "    all_embeddings.append({\n",
    "        'document_id': document_id,\n",
    "        'embeddings': document_embeddings\n",
    "    })\n",
    "\n",
    "# Opslaan van de gegenereerde embeddings\n",
    "embeddings_file = '/home/nena-meijer/PyCharmMiscProject/topic_modelling/BERTopic/embeddings/embeddings_chunks_512_extra_preprocessing.pkl'\n",
    "\n",
    "with open(embeddings_file, 'wb') as f:\n",
    "    pickle.dump(all_embeddings, f)\n",
    "\n",
    "print(f\"Embeddings zijn opgeslagen in: {embeddings_file}\")"
   ],
   "id": "ebe1fc4e0344bb6c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Pre-calculate de embeddings voor de samenvattingen\n",
    "import pickle\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Het model wordt uitgevoerd op: {device}\")\n",
    "\n",
    "# Laad het model en de tokenizer\n",
    "model_name = \"intfloat/multilingual-e5-large-instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "from tqdm import tqdm  # Voor voortgangsbalk\n",
    "import numpy as np\n",
    "\n",
    "def generate_embeddings_batch(docs, tokenizer, model, device, batch_size=32):\n",
    "    model.to(device)\n",
    "    embeddings = []\n",
    "\n",
    "    for i in tqdm(range(0, len(docs), batch_size)):\n",
    "        batch_docs = docs[i:i+batch_size]\n",
    "        inputs = tokenizer(batch_docs, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "        inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            last_hidden_state = outputs.last_hidden_state\n",
    "\n",
    "        attention_mask = inputs[\"attention_mask\"]\n",
    "        last_hidden_state = last_hidden_state.masked_fill(~attention_mask.unsqueeze(-1).bool(), 0)\n",
    "        doc_embeddings = last_hidden_state.sum(dim=1) / attention_mask.sum(dim=1).unsqueeze(-1)\n",
    "        doc_embeddings = F.normalize(doc_embeddings, p=2, dim=1)\n",
    "\n",
    "        embeddings.extend(doc_embeddings.cpu().numpy())\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "# Gebruik:\n",
    "all_embeddings = generate_embeddings_batch(document_text, tokenizer, model, device, batch_size=32)\n",
    "\n",
    "# Opslaan van de gegenereerde embeddings\n",
    "embeddings_file = '/home/nena-meijer/PyCharmMiscProject/topic_modelling/BERTopic/embeddings/embeddings_summaries_overtime.pkl'\n",
    "\n",
    "with open(embeddings_file, 'wb') as f:\n",
    "    pickle.dump(all_embeddings, f)\n",
    "\n",
    "print(f\"Embeddings zijn opgeslagen in: {embeddings_file}\")"
   ],
   "id": "6f13236112aa7a52",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Embeddings normaliseren",
   "id": "e818f9d2cc8d1ff1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pickle\n",
    "from sklearn.preprocessing import normalize\n",
    "import numpy as np\n",
    "\n",
    "print(\"üìÇ Embeddings worden geladen...\")\n",
    "with open('/home/nena-meijer/PyCharmMiscProject/topic_modelling/BERTopic/embeddings/embeddings_summaries.pkl', 'rb') as f:\n",
    "    embeddings_data = pickle.load(f)\n",
    "\n",
    "print(\"üîÑ Embeddings worden samengevoegd in √©√©n matrix...\")\n",
    "all_vectors = []\n",
    "for doc in embeddings_data:\n",
    "    all_vectors.extend(doc['embeddings'])  # Elke doc['embeddings'] is een lijst van numpy-arrays\n",
    "\n",
    "all_vectors = np.vstack(all_vectors)\n",
    "print(f\"‚úÖ Totaal aantal embeddings: {all_vectors.shape[0]}\")\n",
    "\n",
    "# Normaliseer de embeddings in batches en print voortgang\n",
    "batch_size = 1000  # Pas de batchgrootte aan indien nodig\n",
    "normalized_embeddings = []\n",
    "\n",
    "print(\"üìè Embeddings worden genormaliseerd...\")\n",
    "for i in range(0, len(all_vectors), batch_size):\n",
    "    batch = all_vectors[i:i + batch_size]\n",
    "    batch_normalized = normalize(batch, norm='l2')\n",
    "    normalized_embeddings.append(batch_normalized)\n",
    "\n",
    "    # Print voortgang per batch\n",
    "    print(f\"‚úÖ Batch {i // batch_size + 1} genormaliseerd: {i + len(batch)} / {len(all_vectors)} embeddings\")\n",
    "\n",
    "# Zet alle genormaliseerde embeddings weer in √©√©n matrix\n",
    "normalized_embeddings = np.vstack(normalized_embeddings)\n",
    "print(\"‚úÖ Normalisatie voltooid.\")"
   ],
   "id": "4c55d17d115fe3a6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Decodeer de chunks voor BERTopic invoer",
   "id": "e8274e9d0b826429"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pickle\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Laad de tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"intfloat/multilingual-e5-large-instruct\")\n",
    "\n",
    "# Laad de chunks uit het eerder opgeslagen pickle-bestand\n",
    "chunks_file = '/home/nena-meijer/PyCharmMiscProject/topic_modelling/BERTopic/chunks/chunks_512_extra_preprocessing.pkl'\n",
    "\n",
    "with open(chunks_file, 'rb') as f:\n",
    "    loaded_chunks = pickle.load(f)\n",
    "\n",
    "# Genereer document_text per chunk en decodeer de tokens\n",
    "decoded_chunks = []\n",
    "for doc in loaded_chunks:\n",
    "    for chunk in doc['chunks']:\n",
    "        # Decoderen van de tokens naar tekst\n",
    "        decoded_text = tokenizer.decode(chunk, skip_special_tokens=True)\n",
    "        decoded_chunks.append(decoded_text)\n",
    "\n",
    "# Print de eerste 5 decodes chunks om te controleren\n",
    "print(decoded_chunks[:5])"
   ],
   "id": "e6e572238eef4063",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## BERTopic configuratie pipeline modellen",
   "id": "f2f1049450e2f3b8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import cuml\n",
    "from cuml.manifold import UMAP\n",
    "from cuml.cluster import HDBSCAN\n",
    "import cupy as cp\n",
    "import numpy as np\n",
    "from bertopic.representation import KeyBERTInspired, MaximalMarginalRelevance, PartOfSpeech\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "umap_model = UMAP(n_components=5, n_neighbors=20, metric='cosine', min_dist=0.0, random_state=42)\n",
    "\n",
    "hdbscan_model = HDBSCAN(min_cluster_size=20, metric='euclidean', cluster_selection_method='eom', prediction_data=True,\n",
    "                        min_samples=10)\n",
    "\n",
    "vectorizer_model = CountVectorizer(stop_words=\"english\", min_df=2, ngram_range=(1, 4))"
   ],
   "id": "4e7aaca2e35a38df",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Train BERTopic",
   "id": "2326283d7f5eb189"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "embeddings_file = \"/home/nena-meijer/PyCharmMiscProject/topic_modelling/BERTopic/embeddings/embeddings_summaries_preprocessed.pkl\"\n",
    "with open(embeddings_file, 'rb') as f:\n",
    "    embeddings_data = pickle.load(f)\n",
    "\n",
    "all_embeddings = np.array(embeddings_data)\n",
    "\n",
    "# Training & saving the model\n",
    "topic_model = BERTopic(\n",
    "    # Pipeline models\n",
    "    embedding_model=None,\n",
    "    umap_model=umap_model,\n",
    "    hdbscan_model=hdbscan_model,\n",
    "    vectorizer_model=vectorizer_model,\n",
    "\n",
    "    # Hyperparameters\n",
    "    top_n_words=10,\n",
    "    verbose=True,\n",
    "    language=\"multilingual\",\n",
    "    n_gram_range=(1, 4)\n",
    ")\n",
    "\n",
    "topics, probs = topic_model.fit_transform(document_text, all_embeddings)\n",
    "\n",
    "topic_model.save(\"/home/nena-meijer/PyCharmMiscProject/topic_modelling/BERTopic/models/summaries/training_3.pkl\",\n",
    "                 serialization=\"safetensors\", save_ctfidf=True)\n",
    "# get topics\n",
    "topic_model.get_topic_info()"
   ],
   "id": "fa4d773643d1e8de",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Topics over time",
   "id": "491673e66a825d91"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "topics_over_time = topic_model.topics_over_time(document_text, document_dates)",
   "id": "3f8ffad9b462450c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Reduce outliers",
   "id": "b2267f5b693f680d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "new_topics = topic_model.reduce_outliers(document_text, topics, strategy=\"embeddings\", embeddings=all_embeddings)",
   "id": "1dacf5a6d1f3fc4c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "topic_model.update_topics(document_text, topics=new_topics)",
   "id": "45c1d2db6cc9673c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "topic_model.get_topic_info()",
   "id": "33ad135550e7c0ba",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "topic_model.save(\"/home/nena-meijer/PyCharmMiscProject/topic_modelling/BERTopic/models/extra_preprocessing/512/training_3_reduced_outliers_embedddings.pkl\",\n",
    "                 serialization=\"safetensors\", save_ctfidf=True)"
   ],
   "id": "46d3f0dce1840545",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Topic Coherence Measures",
   "id": "833f568f7ea782c4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import gensim.corpora as corpora\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "import gc\n",
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Geheugen vrijmaken\n",
    "torch.cuda.empty_cache()\n",
    "cp.get_default_memory_pool().free_all_blocks()\n",
    "\n",
    "# Preprocess Documents\n",
    "documents = pd.DataFrame({\"Document\": document_text,\n",
    "                          \"ID\": range(len(document_text)),\n",
    "                          \"Topic\": new_topics})\n",
    "documents_per_topic = documents.groupby(['Topic'], as_index=False).agg({'Document': ' '.join})\n",
    "\n",
    "# In plaats van de documenten eerst volledig te preprocessen, maken we nu direct een sparse matrix via de vectorizer\n",
    "# Deze sparse matrix bevat alleen de niet-nul entries (effici√´nt in geheugen)\n",
    "sparse_matrix = topic_model.vectorizer_model.transform(documents_per_topic.Document.values)\n",
    "\n",
    "# Bouw de corpus op basis van de sparse matrix\n",
    "# Voor elke documentrij halen we de indices en bijbehorende aantallen op\n",
    "corpus = [\n",
    "    list(zip(sparse_matrix.getrow(i).indices, sparse_matrix.getrow(i).data))\n",
    "    for i in range(sparse_matrix.shape[0])\n",
    "]\n",
    "\n",
    "# Maak een dictionary op basis van de vectorizer's vocabulaire\n",
    "# vectorizer.vocabulary_ is een dict met token: index; we bouwen de inverse mapping\n",
    "id2word = {v: k for k, v in topic_model.vectorizer_model.vocabulary_.items()}\n",
    "dictionary = corpora.Dictionary()\n",
    "dictionary.id2token = id2word\n",
    "dictionary.token2id = topic_model.vectorizer_model.vocabulary_\n",
    "\n",
    "# Voor de 'texts' parameter gebruiken we de analyzer om de documenten te tokenizen\n",
    "analyzer = topic_model.vectorizer_model.build_analyzer()\n",
    "texts = [analyzer(doc) for doc in documents_per_topic.Document.values]\n",
    "\n",
    "# Haal de topic-woorden op uit BERTopic (elke topic als een lijst van woorden)\n",
    "topic_words = [\n",
    "    [word for word, _ in topic_model.get_topic(topic)]\n",
    "    for topic in range(len(set(new_topics)) - 1)\n",
    "]\n",
    "\n",
    "# Optioneel: Ruim tussentijdse objecten op als je deze niet meer nodig hebt\n",
    "del documents_per_topic\n",
    "gc.collect()\n",
    "\n",
    "# Bereken de topic coherence met de sparse corpus\n",
    "coherence_model_umass = CoherenceModel(\n",
    "    topics=topic_words,\n",
    "    texts=texts,\n",
    "    corpus=corpus,\n",
    "    dictionary=dictionary,\n",
    "    coherence='c_npmi'\n",
    ")\n",
    "coherence_umass = coherence_model_umass.get_coherence()\n",
    "print(\"Coherence score algemeen:\", coherence_umass)\n",
    "\n",
    "coherence_per_topic = coherence_model_umass.get_coherence_per_topic()\n",
    "for i, (score, words) in enumerate(zip(coherence_per_topic, topic_words)):\n",
    "    print(f\"Topic {i}: {words[:5]} ‚Üí Coherence: {score:.4f}\")\n",
    "\n",
    "del corpus\n",
    "del dictionary\n",
    "gc.collect()"
   ],
   "id": "b06dd2606d92d5c6",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
