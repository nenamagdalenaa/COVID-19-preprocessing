{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# LDA",
   "id": "35dd81dead9f895f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ],
   "id": "51b16fcf19df0d3b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Documenten voorbereiden\n",
    "De chunks worden aan het model gegeven"
   ],
   "id": "204b451e1684515b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pickle\n",
    "\n",
    "chunks_file = \"/home/nena-meijer/PyCharmMiscProject/topic_modelling/BERTopic/chunks/chunks_decoded.pkl\"\n",
    "\n",
    "with open(chunks_file, 'rb') as f:\n",
    "    docs = pickle.load(f)\n",
    "\n",
    "print(len(docs))\n",
    "print(docs[0][:500])"
   ],
   "id": "a5ddecd7d6198725",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Preprocess en vectorize de chunks",
   "id": "d636add5c7af3456"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Tokenize the documents.\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# Split the documents into tokens.\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "for idx in range(len(docs)):\n",
    "    docs[idx] = docs[idx].lower()  # Convert to lowercase.\n",
    "    docs[idx] = tokenizer.tokenize(docs[idx])  # Split into words.\n",
    "\n",
    "# Remove numbers, but not words that contain numbers.\n",
    "docs = [[token for token in doc if not token.isnumeric()] for doc in docs]\n",
    "\n",
    "# Remove words that are only one character.\n",
    "docs = [[token for token in doc if len(token) > 1] for doc in docs]"
   ],
   "id": "b41bca25deb2fe9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Lemmatize the documents.\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "docs = [[lemmatizer.lemmatize(token) for token in doc] for doc in docs]"
   ],
   "id": "176a37e4c234a5a4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Bigrams zijn sets van twee adjacent woorden. Door bigrams te gebruiken krijgen we representaties zoals \"machine_learning\" in onze output, in plaats van alleen maar \"machine\" en \"learning\".",
   "id": "7af31c75a2fa73f6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Compute bigrams.\n",
    "from gensim.models import Phrases\n",
    "\n",
    "# Add bigrams and trigrams to docs (only ones that appear 20 times or more).\n",
    "bigram = Phrases(docs, min_count=20)\n",
    "for idx in range(len(docs)):\n",
    "    for token in bigram[docs[idx]]:\n",
    "        if '_' in token:\n",
    "            # Token is a bigram, add to document.\n",
    "            docs[idx].append(token)"
   ],
   "id": "27b413c6229f9f73",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Remove rare and common tokens.\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "# Create a dictionary representation of the documents.\n",
    "dictionary = Dictionary(docs)\n",
    "\n",
    "# Filter out words that occur less than 20 documents, or more than 50% of the documents.\n",
    "dictionary.filter_extremes(no_below=20, no_above=0.5)"
   ],
   "id": "ebbb3d21a91b3933",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Bag-of-words representation of the documents.\n",
    "corpus = [dictionary.doc2bow(doc) for doc in docs]"
   ],
   "id": "fd1995aea00c9dcf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print('Number of unique tokens: %d' % len(dictionary))\n",
    "print('Number of documents: %d' % len(corpus))"
   ],
   "id": "b186e74411f6d200",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Train LDA model.\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "# Set training parameters.\n",
    "num_topics = 175\n",
    "chunksize = 2000\n",
    "passes = 20\n",
    "iterations = 400\n",
    "eval_every = None  # Don't evaluate model perplexity, takes too much time.\n",
    "\n",
    "# Make an index to word dictionary.\n",
    "temp = dictionary[0]  # This is only to \"load\" the dictionary.\n",
    "id2word = dictionary.id2token\n",
    "\n",
    "model = LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=id2word,\n",
    "    chunksize=chunksize,\n",
    "    alpha=0.8,\n",
    "    eta=0.5,\n",
    "    iterations=iterations,\n",
    "    num_topics=num_topics,\n",
    "    passes=passes,\n",
    "    eval_every=eval_every\n",
    ")"
   ],
   "id": "c7f9994575c89297",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Sla het model op\n",
    "model.save(\"/home/nena-meijer/PyCharmMiscProject/topic_modelling/LDA/models/training_5/lda_model.gensim\")"
   ],
   "id": "7fe79509d5f26441",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for topic in model.print_topics():\n",
    "    print(topic)"
   ],
   "id": "34f086358c7051ba",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "coherence_model_umass = CoherenceModel(model=model, corpus=corpus, dictionary=dictionary, coherence='u_mass')\n",
    "coherence_model_cv = CoherenceModel(model=model, texts=docs, corpus=corpus, dictionary=dictionary, coherence='c_v')\n",
    "coherence_model_npmi = CoherenceModel(model=model, texts=docs, corpus=corpus, dictionary=dictionary, coherence='c_npmi')\n",
    "\n",
    "print(f\"u_mass Coherence: {coherence_model_umass.get_coherence()}\")\n",
    "print(f\"c_v Coherence: {coherence_model_cv.get_coherence()}\")\n",
    "print(f\"c_npmi Coherence: {coherence_model_npmi.get_coherence()}\")"
   ],
   "id": "6201e7da186cd1a4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Aantal topics en topwoorden per topic\n",
    "num_topics = model.num_topics\n",
    "top_words_per_topic = 10  # Aantal topwoorden per topic\n",
    "\n",
    "# Haal de topwoorden per topic\n",
    "for topic_num in range(num_topics):\n",
    "    print(f\"Topic #{topic_num}:\")\n",
    "    top_words = model.show_topic(topic_num, top_words_per_topic)\n",
    "    for word, weight in top_words:\n",
    "        print(f\"  {word}: {weight:.4f}\")\n",
    "\n",
    "# Optioneel: Maak een visualisatie van de topwoorden per topic\n",
    "for topic_num in range(num_topics):\n",
    "    top_words = model.show_topic(topic_num, top_words_per_topic)\n",
    "    words = [word for word, _ in top_words]\n",
    "    weights = [weight for _, weight in top_words]\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(words, weights, color='skyblue')\n",
    "    plt.title(f\"Topic #{topic_num} - Top {top_words_per_topic} Words\")\n",
    "    plt.xlabel('Weight')\n",
    "    plt.show()\n"
   ],
   "id": "d26a02ed93b9c71b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Bereken de topic-distributie per document\n",
    "document_topics = model.get_document_topics(corpus)\n",
    "\n",
    "# Visualiseer de verdeling van topics per document\n",
    "topic_proportions = []\n",
    "for doc in document_topics:\n",
    "    topic_proportions.append([t[1] for t in doc])  # Haal de gewichten van de topics op\n",
    "\n",
    "# Converteer naar numpy array voor gemakkelijke verwerking\n",
    "topic_proportions = np.array(topic_proportions)\n",
    "\n",
    "# Plot de verdeling van topics per document\n",
    "plt.figure(figsize=(10, 6))\n",
    "for i in range(num_topics):\n",
    "    plt.plot(topic_proportions[:, i], label=f\"Topic #{i}\")\n",
    "\n",
    "plt.title(\"Topic Distribution per Document\")\n",
    "plt.xlabel(\"Document Index\")\n",
    "plt.ylabel(\"Topic Proportion\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ],
   "id": "9bf58329143eab17",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Genereren van samenvattingen van de topics\n",
    "topic_summaries = []\n",
    "for topic_num in range(num_topics):\n",
    "    top_words = model.show_topic(topic_num, top_words_per_topic)\n",
    "    topic_words = [word for word, _ in top_words]\n",
    "    topic_summary = ', '.join(topic_words)\n",
    "    topic_summaries.append(f\"Topic #{topic_num}: {topic_summary}\")\n",
    "\n",
    "# Print de samenvattingen\n",
    "for summary in topic_summaries:\n",
    "    print(summary)\n"
   ],
   "id": "a89cca968ddc171a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "5bab4acbf90d4bee",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
